1.	What is a Hive Variable and What Is It Used For?
	Referenced by Hive scripting languages, a Hive variable is created in the Hive environment and uses the source command. Once Hive queries begin executing, a Hive variable provides values to queries. 

2. 	What Are the Different Modes in the Hive?
	This may seem like an easy question, but again, sometimes interviewers like to ask these basic questions to see how confident you are when it comes to your Hive knowledge. Answer by saying that Hive can sometimes operate in two modes, which are MapReduce mode and local mode. Explain that this depends on the size of the DataNodes in Hadoop. 

3.	What is Hive Bucketing?
	When performing queries on large datasets in Hive, bucketing can offer better structure to Hive tables. You’ll also want to take your answer a step further by explaining some of the specific bucketing features, as well as some of the advantages of bucketing in Hive. For example, bucketing can give programmers more flexibility when it comes to record-keeping and can make it easier to debug large datasets when needed.

4.	What is Hive Composed Of?
	Hive is made up of three main components: Hive Services, Hive Clients, and Hive Storage and Computing. You should also briefly explain to your interviewer what each component is capable of and the differences between each part.
	
5.	What Are the Main Components of Hive Architecture?
	You’ll first want to answer this question by naming each of the main components: Driver, User Interface, Execute Engine, Compiler, and Megastore. You’ll really demonstrate your Hive knowledge to your interviewer if you’re able to explain the capabilities of each component as well. 

6.	What Options Are Available When It Comes to Attaching Applications to the Hive Server?
	Explain the three different ways (Thrift Client, JDBC Driver, and ODBC Driver) you can connect applications to the Hive Server. You’ll also want to explain the purpose for each option: for example, using JDBC will support the JDBC protocol.

7.	What Variations of Tables Are Available in Hive?
	This is a fairly straightforward question for someone experienced in Hive, so it’s important to know the answer without hesitation: The two types of tables are managed tables and external tables.

8.	What Are Partitions?
	In Hive, tables are organized and divided into partitions. You’ll want to include this in your answer, as well as explain why partitions are useful in Hive. 

9.	What File Formats and Applications Does Hive Support?
	The answer to this question will include a lot of information, so it’s important to be prepared to list as many supported file formats and applications as possible. Applications written in C++, Python, Java, PHP, and Ruby are generally supported in Hive. When it comes to filing formats, Hive supports text file formats by default but also supports binary file formats, such as Avro data, ORC, Sequence, and Parquet files.
	
10.	Why Hive doesnot store Metadata information in HDFS?
	Hive stores metadata information in the metastore using RDBMS instead of HDFS. The main reason for choosing RDBMS is to achieve low latency because HDFS read/write operations are time consuming processes.

11.	What is SAMPLING in HIVE?
	Sampling allows users to take a subset of dataset and analyze it, without having to analyze the entire data set. If a representative sample is used, then a query can return meaningful results as well as finish quicker and consume fewer compute resources.
	Hive offers a built-in TABLESAMPLE clause that allows you to sample your tables. TABLESAMPLE can sample at various granularity levels – it can return only subsets of buckets (bucket sampling), or HDFS blocks (block sampling), or only first N records from each input split. Alternatively, you can implement your own UDF that filters out records according to your sampling algorithm.
	
12.	What is Vectorization in HIVE?
	Vectorization allows Hive to process a batch of rows together instead of processing one row at a time. Each batch consists of a column vector which is usually an array of primitive types. Operations are performed on the entire column vector, which improves the instruction pipelines and cache usage. To enable vectorization, set this configuration parameter SET hive.vectorized.execution.enabled=true.
	
13.	What are different "Input Format Selection Files" in HIVE?
	Input formats play a critical role in Hive performance. 
	For example JSON, the text type of input formats, is not a good choice for a large production system where data volume is really high.
	These type of readable formats actually take a lot of space and have some overhead of parsing ( e.g JSON parsing ). 
	To address these problems, Hive comes with columnar input formats like RCFile, ORC etc. 
	Columnar formats allow you to reduce the read operations in analytics queries by allowing each column to be accessed individually. 
	There are some other binary formats like Avro, sequence files, Thrift and ProtoBuf, which can be helpful in various use cases too.
	
14.	What are the uses of " Map join" in HIVE?
	Map joins are really efficient if a table on the other side of a join is small enough to fit in the memory.
	Hive supports a parameter, "hive.auto.convert.join", which when it’s set to “true” suggests that Hive try to map join automatically.
	When using this parameter, be sure the auto convert is enabled in the Hive environment.
	
15.	How to "De-normalizing data" in HIVE?
	Normalization is a standard process used to model your data tables with certain rules to deal with redundancy of data and anomalies. 
	In simpler words, if you normalize your data sets, you end up creating multiple relational tables which can be joined at the run time to produce the results. 
	Joins are expensive and difficult operations to perform and are one of the common reasons for performance issues (Tweet this). 
	Because of that, it’s a good idea to avoid highly normalized table structures because they require join queries to derive the desired metrics.
